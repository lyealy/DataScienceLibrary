{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: Python Data Science Handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%autosave 9999\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wrong way of model validation\n",
    "# load iris method one\n",
    "from sklearn.datasets import load_iris\n",
    "iris1 = load_iris()\n",
    "X1 = iris1.data\n",
    "y1 = iris1.target\n",
    "print(X1.shape,y1.shape,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall method two\n",
    "iris2 = sns.load_dataset('iris')\n",
    "iris2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris2['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2n(x):\n",
    "    if x=='setosa':\n",
    "        return 0\n",
    "    elif x=='versicolor':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "iris2['species_n'] = iris2['species'].map(s2n)\n",
    "X2 = iris2.values[:,0:4]\n",
    "y2 = iris2.values[:,5]\n",
    "print(X2.shape,y2.shape,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kNN with n_eighbors =1\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X1,y1)\n",
    "y_pred = model.predict(X1)\n",
    "accuracy_score(y1,y_pred)\n",
    "# training and testing by using the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state = 0, train_size = 0.5)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by splitting, we lost some data in the training set, \n",
    "# this may cause problem when the original trainging set is small\n",
    "y_pred1 = model.fit(X_train,y_train).predict(X_test)\n",
    "y_pred2 = model.fit(X_test,y_test).predict(X_train)\n",
    "accuracy_score(y_test,y_pred1), accuracy_score(y_train,y_pred2)\n",
    "# this is called cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(model,X1,y1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import LeaveOneOut\n",
    "score = cross_val_score(model,X1,y1,cv=LeaveOneOut(len(y1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curves in Scikit-Learn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2,**kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),LinearRegression(**kwargs))\n",
    "\n",
    "def make_data(N,err=1.0, rseed=1):\n",
    "    np.random.seed(rseed)\n",
    "    X = np.random.rand(N,1)**2\n",
    "    y = 10 - 1./(X.ravel()+0.1)\n",
    "    if err>0:\n",
    "        y = y + err*np.random.randn(N)\n",
    "    return X,y\n",
    "X,y = make_data(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "X_test = np.linspace(-0.1,1.1,500).reshape(500,1)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X.ravel(),y,color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1,5,9,30]:\n",
    "    y_test = PolynomialRegression(degree).fit(X,y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(),y_test,label='degree={}'.format(degree))\n",
    "plt.xlim(-0.1,1.0)\n",
    "plt.ylim(-2,12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "degree = np.arange(0,21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(),X,y,\n",
    "                                         'polynomialfeatures__degree',degree,cv=7)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(degree,np.median(train_score,1), color='blue',label='training score')\n",
    "plt.plot(degree,np.median(val_score,1), color='red',label='validation score')\n",
    "plt.legend('best')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X.ravel(),y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X,y).predict(X_test)\n",
    "plt.plot(X_test.ravel(),y_test)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal model will depend on the size of your training data\n",
    "X2, y2 = make_data(200)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X2.ravel(),y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(0,21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(),X2,y2,\n",
    "                                         'polynomialfeatures__degree',degree,cv=7)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(degree,np.median(train_score2,1), color='blue',label='training score')\n",
    "plt.plot(degree,np.median(val_score2,1), color='red',label='validation score')\n",
    "plt.plot(degree,np.median(train_score,1), color='blue',alpha=0.3,linestyle='dashed')\n",
    "plt.plot(degree,np.median(val_score,1), color='red',alpha=0.3,linestyle='dashed')\n",
    "plt.legend('best')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2,9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
    "                                         X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model via Grid Search\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n",
    "grid.fit(X, y);\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test, hold=True);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
